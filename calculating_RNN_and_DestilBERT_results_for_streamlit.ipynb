{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>review text</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>minimal f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BoW encoding</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>processed</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tfidf encoding</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.488</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GloVe encoding</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.553</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Word2Vec encoding</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.568</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM with GloVe encoding</td>\n",
       "      <td>unbalanced</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LSTM with GloVe encoding</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DestilBERT</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>processed</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DestilBERT</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>unprocessed</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BERT</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>unprocessed</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nlptown unchanged</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>unprocessed</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nlptown finetuned</td>\n",
       "      <td>undersampled</td>\n",
       "      <td>unprocessed</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Method       Dataset  review text Accuracy  \\\n",
       "0               BoW encoding    unbalanced    processed      nan   \n",
       "1             Tfidf encoding    unbalanced    processed    0.488   \n",
       "2             GloVe encoding    unbalanced    processed    0.553   \n",
       "3          Word2Vec encoding    unbalanced    processed    0.568   \n",
       "4   LSTM with GloVe encoding    unbalanced    processed     0.76   \n",
       "5   LSTM with GloVe encoding  undersampled    processed     0.55   \n",
       "6                 DestilBERT  undersampled    processed     0.59   \n",
       "7                 DestilBERT  undersampled  unprocessed     0.62   \n",
       "8                       BERT  undersampled  unprocessed     0.62   \n",
       "9          nlptown unchanged  undersampled  unprocessed     0.54   \n",
       "10         nlptown finetuned  undersampled  unprocessed     0.62   \n",
       "\n",
       "   minimal f1 score  \n",
       "0               nan  \n",
       "1               nan  \n",
       "2               nan  \n",
       "3               nan  \n",
       "4              0.05  \n",
       "5               0.4  \n",
       "6              0.44  \n",
       "7              0.48  \n",
       "8              0.49  \n",
       "9               0.4  \n",
       "10             0.49  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Results dictionary data\n",
    "data = {\n",
    "    'Method': ['BoW encoding', 'Tfidf encoding', 'GloVe encoding', 'Word2Vec encoding', 'LSTM with GloVe encoding', 'LSTM with GloVe encoding', 'DestilBERT', 'DestilBERT', 'BERT', 'nlptown unchanged', 'nlptown finetuned'],\n",
    "    'Dataset': ['unbalanced', 'unbalanced', 'unbalanced', 'unbalanced', 'unbalanced', 'undersampled', 'undersampled', 'undersampled', 'undersampled', 'undersampled', 'undersampled'],\n",
    "    'review text': ['processed', 'processed', 'processed', 'processed', 'processed', 'processed', 'processed', 'unprocessed', 'unprocessed', 'unprocessed', 'unprocessed'],\n",
    "    'Accuracy': ['nan', 0.488, 0.553, 0.568, 0.76, 0.55, 0.59, 0.62, 0.62, 0.54, 0.62],\n",
    "    'minimal f1 score': ['nan', 'nan', 'nan', 'nan', 0.05, 0.40, 0.44, 0.48, 0.49, 0.40, 0.49]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "df=pd.read_csv(r\"data\\data_undersampled.csv\")\n",
    "df_text = df[['rating', 'text', 'text_processed']]\n",
    "\n",
    "# Path to saved model and tokenizer\n",
    "model_save_path = \"finetuned_destilbert_unpr_model\"\n",
    "tokenizer_save_path = \"finetuned_destilbert_unpr_tokenizer\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "def load_model_and_tokenizer():\n",
    "    # Load the model and tokenizer\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_save_path)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_save_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "\n",
    "# Select the column to use as input \n",
    "input_column = 'text'  \n",
    "     \n",
    "\n",
    "# Tokenization and Dataset Preparation\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        text = row[input_column]\n",
    "        rating = row['rating'] -1\n",
    "        tokens = self.tokenizer(\n",
    "            text, \n",
    "            max_length=self.max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': tokens['input_ids'].squeeze(),\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(rating, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def prepare_datasets(df, _tokenizer, max_length):\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_dataset = ReviewDataset(train_df, tokenizer, max_length)\n",
    "    test_dataset = ReviewDataset(test_df, tokenizer, max_length)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "max_length = 128\n",
    "      \n",
    "\n",
    "# Prepare DataSet\n",
    "train_dataset, test_dataset = prepare_datasets(df, tokenizer, max_length)\n",
    "\n",
    "\n",
    "def make_predictions(_model, test_dataset):\n",
    "                        \n",
    "    # Make predictions \n",
    "    model.eval()  \n",
    "    predictions1 = []\n",
    "    true_labels1 = []\n",
    "\n",
    "    # Iterate over test dataset to make predictions\n",
    "    for batch in test_dataset:\n",
    "        input_ids = batch['input_ids'].unsqueeze(0)  \n",
    "        attention_mask = batch['attention_mask'].unsqueeze(0)  \n",
    "        labels = batch['labels'].unsqueeze(0)\n",
    "    \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "        # Get predicted class\n",
    "        logits = outputs.logits\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "        predictions1.append(predicted_class)\n",
    "        true_labels1.append(labels.item())\n",
    "          \n",
    "    return predictions1, true_labels1\n",
    "\n",
    "predictions, true_labels = make_predictions(model, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save predictions and true_labels to a pickle file\n",
    "with open('predictions_and_labels_DB.pkl', 'wb') as f:\n",
    "    pickle.dump({'predictions': predictions, 'true_labels': true_labels}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "     \n",
    "texts = df['text_processed'].astype(str).tolist()  \n",
    "ratings = df['rating'].astype(int).values  \n",
    "     \n",
    "# Tokenize and Convert Text to Sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  \n",
    "\n",
    "X = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_length = 60  \n",
    "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode ratings\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(ratings)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load GloVe Embeddings\n",
    "embedding_dim = 300\n",
    "glove_file = \"glove.6B.300d.txt\"  \n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_file, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = vector\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "     \n",
    "\n",
    "model1 = load_model('rnn_glove_rating_under.keras')\n",
    "\n",
    "# make predictions\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "test_pred_class= y_pred.argmax(axis = 1)\n",
    "y_test_class = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test_pred_class and y_test_class to a pickle file\n",
    "with open('predictions_and_labels_RNN.pkl', 'wb') as f:\n",
    "    pickle.dump({'test_pred_class': test_pred_class, 'y_test_class': y_test_class}, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supply_chain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
