{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import stats\n",
    "from IPython.display import display, HTML\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, auc, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.api import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 3)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "\n",
    "train_df = pd.read_csv('data/train_trustpilot_3.csv', engine='python')\n",
    "df_trustpilot = pd.read_csv('data/data_trustpilot_3.csv', engine='python')\n",
    "test_df = pd.read_csv('data/test_trustpilot_3.csv', engine='python')\n",
    "\n",
    "display(train_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First check\n",
    "train_df['local_date_posted'] = pd.to_datetime(train_df['local_date_posted'])\n",
    "\n",
    "print(train_df.columns)\n",
    "display(train_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 1 - HEATMAP & MULTICOLLINEARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split set into numeric, categorical and datetime variables\n",
    "#Provide statistical metrics for numeric variables\n",
    "\n",
    "numeric_variables = train_df.select_dtypes(include = ['int', 'float'])\n",
    "categorical_variables = train_df.select_dtypes(include=['object']) \n",
    "datetime_variables = train_df.select_dtypes(include=['datetime64'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numeric variables\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = numeric_variables.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Numeric Variables')\n",
    "plt.show()\n",
    "\n",
    "print(\"Highly correlated pairs (>0.7) with analysis:\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            correlation = correlation_matrix.iloc[i, j]\n",
    "            \n",
    "            corr_with_target1 = abs(correlation_matrix.loc[var1, 'rating'])\n",
    "            corr_with_target2 = abs(correlation_matrix.loc[var2, 'rating'])\n",
    "            \n",
    "            print(f\"{var1} & {var2}: {correlation:.3f}\")\n",
    "            print(f\"Correlation with rating: {var1}: {corr_with_target1:.3f}, {var2}: {corr_with_target2:.3f}\")\n",
    "            print(f\"Recommendation: Keep {var1 if corr_with_target1 > corr_with_target2 else var2} \"\n",
    "                  f\"(stronger correlation with target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 2 - PEARSON AND SPEARMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pearson and Spearman\n",
    "\n",
    "def analyze_correlation_with_rating(column_name):\n",
    "    if column_name not in train_df.columns:\n",
    "        return f\"Error: Column '{column_name}' not found in dataframe\"\n",
    "    \n",
    "    # Calculate correlations\n",
    "    pearson_coeff, pearson_p = pearsonr(train_df['rating'], train_df[column_name])\n",
    "    spearman_coeff, spearman_p = spearmanr(train_df['rating'], train_df[column_name])\n",
    "    \n",
    "    # Format output\n",
    "    output = []\n",
    "    output.append(f\"Correlation Analysis: rating vs {column_name}\")\n",
    "    output.append(\"=\" * 50)\n",
    "    output.append(\"Hypothesis:\")\n",
    "    output.append(\"H0: The two variables are uncorrelated\")\n",
    "    output.append(\"H1: The two variables are correlated\")\n",
    "    output.append(\"Results:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    \n",
    "    # Pearson Analysis\n",
    "    output.append(f\"Pearson coefficient: {pearson_coeff}\")\n",
    "    if abs(pearson_coeff) < 0.1:\n",
    "        strength = \"extremely weak/negligible\"\n",
    "    elif abs(pearson_coeff) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(pearson_coeff) < 0.5:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    direction = \"negative\" if pearson_coeff < 0 else \"positive\"\n",
    "    output.append(f\"Negative sign indicates an inverse relationship but as the value is {strength}, \"\n",
    "                 f\"this indicates a{strength} {direction} correlation.\")\n",
    "    \n",
    "    output.append(f\"Pearson p_value: {pearson_p}\")\n",
    "    if pearson_p < 0.05:\n",
    "        output.append(\"Value is smaller than the common significance level of 0.05. \"\n",
    "                     f\"This indicates that the correlation, although {strength}, is statistically significant.\")\n",
    "    else:\n",
    "        output.append(\"Value is larger than the common significance level of 0.05. \"\n",
    "                     \"This indicates that the correlation is not statistically significant.\")\n",
    "    \n",
    "    # Spearman Analysis\n",
    "    output.append(f\"Spearman coefficient: {spearman_coeff}\")\n",
    "    if abs(spearman_coeff) < 0.1:\n",
    "        strength = \"extremely weak/negligible\"\n",
    "    elif abs(spearman_coeff) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(spearman_coeff) < 0.5:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    output.append(f\"Negative sign indicates an inverse relationship and as the value is {strength}, \"\n",
    "                 f\"this indicates a {strength} {direction} correlation.\")\n",
    "    \n",
    "    output.append(f\"Spearman p_value: {spearman_p}\")\n",
    "    if spearman_p < 0.05:\n",
    "        output.append(\"Value is smaller than the common significance level of 0.05. \"\n",
    "                     f\"This indicates that the correlation, although {strength}, is statistically significant.\")\n",
    "    else:\n",
    "        output.append(\"Value is larger than the common significance level of 0.05. \"\n",
    "                     \"This indicates that the correlation is not statistically significant.\")\n",
    "    \n",
    "    # Conclusion\n",
    "    output.append(\"Conclusion:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    if pearson_p < 0.05 or spearman_p < 0.05:\n",
    "        output.append(\"H0 is rejected\")\n",
    "        output.append(\"H1 is confirmed\")\n",
    "    else:\n",
    "        output.append(\"Failed to reject H0\")\n",
    "        \n",
    "    # Additional insights\n",
    "    if abs(abs(spearman_coeff) - abs(pearson_coeff)) > 0.1:\n",
    "        output.append(f\"The difference between Pearson ({pearson_coeff:.3f}) and \"\n",
    "                     f\"Spearman ({spearman_coeff:.3f}) suggests a non-linear relationship\")\n",
    "    \n",
    "    # Practical interpretation\n",
    "    output.append(f\"As {column_name} increases, ratings tend to \"\n",
    "                 f\"{'decrease' if pearson_coeff < 0 else 'increase'}\")\n",
    "    output.append(\"While statistically significant, the relationship is \"\n",
    "                 f\"{strength}\")\n",
    "    output.append(\"Keep for model training of a rating model\")\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "\n",
    "# Get list of numeric columns excluding 'rating'\n",
    "numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'rating']\n",
    "\n",
    "print(\"Starting correlation analysis for all numeric variables...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Loop through each numeric column\n",
    "for column in numeric_columns:\n",
    "    display(HTML(f\"<pre>{analyze_correlation_with_rating(column)}</pre>\"))\n",
    "    display(HTML(\"<hr>\")) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 3 - MUTUAL INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mutual_information_with_rating(column_name):\n",
    "    if column_name not in train_df.columns:\n",
    "        return f\"Error: Column '{column_name}' not found in dataframe\"\n",
    "    \n",
    "    # Calculate mutual information\n",
    "    mi_score = mutual_info_regression(train_df[[column_name]], train_df['rating'], random_state=42)[0]\n",
    "    \n",
    "    # Format output\n",
    "    output = []\n",
    "    output.append(f\"Mutual Information Analysis: rating vs {column_name}\")\n",
    "    output.append(\"=\" * 50)\n",
    "    output.append(\"Interpretation:\")\n",
    "    output.append(\"Mutual Information measures the dependency between variables (including non-linear relationships)\")\n",
    "    output.append(\"Score ranges from 0 (no mutual information) to ln(n_classes) (perfect mutual information)\")\n",
    "    output.append(\"Results:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    \n",
    "    # MI Analysis\n",
    "    output.append(f\"Mutual Information score: {mi_score:.4f}\")\n",
    "    if mi_score < 0.01:\n",
    "        strength = \"extremely weak/negligible\"\n",
    "    elif mi_score < 0.05:\n",
    "        strength = \"weak\"\n",
    "    elif mi_score < 0.2:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    \n",
    "    output.append(f\"This indicates a {strength} dependency between {column_name} and rating\")\n",
    "    \n",
    "    # Conclusion\n",
    "    output.append(\"Conclusion:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    if mi_score > 0.01:\n",
    "        output.append(f\"There is a {strength} statistical dependency between {column_name} and rating\")\n",
    "        output.append(\"Feature could be valuable for predictive modeling\")\n",
    "    else:\n",
    "        output.append(\"There is negligible statistical dependency between the variables\")\n",
    "        output.append(\"Consider dropping this feature for modeling\")\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "# Get list of numeric columns excluding 'rating'\n",
    "numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'rating']\n",
    "\n",
    "print(\"Starting mutual information analysis for all numeric variables...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Loop through each numeric column\n",
    "for column in numeric_columns:\n",
    "    display(HTML(f\"<pre>{analyze_mutual_information_with_rating(column)}</pre>\"))\n",
    "    display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 4 - ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA\n",
    "\n",
    "def analyze_anova_with_rating(df):\n",
    "    attributes = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    attributes = [col for col in attributes if col != 'rating']\n",
    "    \n",
    "    # Store results and selected features\n",
    "    results = {}\n",
    "    feat_select = []\n",
    "    \n",
    "    print(\"ANOVA Analysis: Features vs Rating\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze each feature\n",
    "    for feature in attributes:\n",
    "        try:\n",
    "            lm = ols('rating ~ {}'.format(feature), data=df).fit()\n",
    "            table = anova_lm(lm)\n",
    "            p_value = table['PR(>F)'].iloc[0]\n",
    "            \n",
    "            # If significant, add to selected features\n",
    "            if p_value <= 0.05:\n",
    "                feat_select.append(feature)\n",
    "                results[feature] = p_value\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {feature}: {str(e)}\")\n",
    "    \n",
    "    # Create and display results table\n",
    "    if feat_select:\n",
    "        results_df = pd.DataFrame({\n",
    "            'Feature': feat_select,\n",
    "            'P-value': [results[f] for f in feat_select]\n",
    "        }).sort_values('P-value')\n",
    "        \n",
    "        print(\"Selected Features (p ≤ 0.05):\")\n",
    "        print(results_df)\n",
    "    \n",
    "    return feat_select\n",
    "\n",
    "selected_features = analyze_anova_with_rating(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANOVA \n",
    "\n",
    "def analyze_anova_with_rating(df):\n",
    "    attributes = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    attributes = [col for col in attributes if col != 'rating']\n",
    "    \n",
    "    results = {}\n",
    "    feat_select = []\n",
    "    output_text = []\n",
    "    \n",
    "    output_text.append(\"ANOVA Analysis: Features vs Rating\")\n",
    "    output_text.append(\"=\" * 50)\n",
    "    \n",
    "    for feature in attributes:\n",
    "        try:\n",
    "            lm = ols('rating ~ {}'.format(feature), data=df).fit()\n",
    "            table = anova_lm(lm)\n",
    "            p_value = table['PR(>F)'].iloc[0]\n",
    "            f_stat = table['F'].iloc[0]\n",
    "            \n",
    "            # Calculate effect size (eta-squared)\n",
    "            ss_between = table['sum_sq'][0]\n",
    "            ss_total = ss_between + table['sum_sq'][1]\n",
    "            eta_squared = ss_between / ss_total\n",
    "            \n",
    "            # Store results\n",
    "            results[feature] = {\n",
    "                'p_value': p_value,\n",
    "                'f_stat': f_stat,\n",
    "                'eta_squared': eta_squared\n",
    "            }\n",
    "            \n",
    "            # Interpret effect size\n",
    "            if eta_squared < 0.01:\n",
    "                effect_strength = \"negligible\"\n",
    "            elif eta_squared < 0.06:\n",
    "                effect_strength = \"small\"\n",
    "            elif eta_squared < 0.14:\n",
    "                effect_strength = \"medium\"\n",
    "            else:\n",
    "                effect_strength = \"large\"\n",
    "            \n",
    "            # Feature selection\n",
    "            if p_value <= 0.05:\n",
    "                feat_select.append(feature)\n",
    "            \n",
    "            # Output for each feature\n",
    "            output_text.append(f\"Analysis for: {feature}\")\n",
    "            output_text.append(\"-\" * 30)\n",
    "            output_text.append(f\"F-statistic: {f_stat:.4f}\")\n",
    "            output_text.append(f\"P-value: {p_value:.4e}\")\n",
    "            output_text.append(f\"Effect size (η²): {eta_squared:.4f}\")\n",
    "            output_text.append(f\"Effect strength: {effect_strength}\")\n",
    "            output_text.append(f\"Selected for model: {'Yes' if p_value <= 0.05 else 'No'}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            output_text.append(f\"Error analyzing {feature}: {str(e)}\")\n",
    "    \n",
    "    output_text.append(\"\" + \"=\" * 50)\n",
    "    output_text.append(\"Summary:\")\n",
    "    output_text.append(f\"Total features analyzed: {len(attributes)}\")\n",
    "    output_text.append(f\"Features selected: {len(feat_select)}\")\n",
    "    output_text.append(\"Selected features:\")\n",
    "    for feat in feat_select:\n",
    "        p_value = results[feat]['p_value']\n",
    "        eta = results[feat]['eta_squared']\n",
    "        output_text.append(f\"- {feat}: p={p_value:.4e}, η²={eta:.4f}\")\n",
    "    \n",
    "    print(\"\".join(output_text))\n",
    "    return results, feat_select\n",
    "\n",
    "results, selected_features = analyze_anova_with_rating(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 5 - VARIANCE THRESHOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variance Threshold\n",
    "\n",
    "def analyze_variance_threshold(threshold=0.0):\n",
    "   # Get numeric columns only\n",
    "   numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "   \n",
    "   # Calculate variance for each feature\n",
    "   variances = numeric_data.var()\n",
    "   \n",
    "   output = []\n",
    "   output.append(\"Variance Threshold Analysis\")\n",
    "   output.append(\"=\" * 50)\n",
    "   output.append(\"Feature Variances:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   \n",
    "   # Sort variances in descending order\n",
    "   sorted_variances = variances.sort_values(ascending=False)\n",
    "   for feature, var in sorted_variances.items():\n",
    "       output.append(f\"{feature}: {var:.6f}\")\n",
    "       if var <= threshold:\n",
    "           output.append(f\"WARNING: {feature} has variance <= {threshold}\")\n",
    "   \n",
    "   # Identify low variance features\n",
    "   low_variance_features = variances[variances <= threshold].index.tolist()\n",
    "   \n",
    "   output.append(\"Conclusion:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   if low_variance_features:\n",
    "       output.append(f\"Features with variance <= {threshold}:\")\n",
    "       for feature in low_variance_features:\n",
    "           output.append(f\"- {feature}\")\n",
    "       output.append(\"Consider removing these low-variance features\")\n",
    "   else:\n",
    "       output.append(f\"All features have variance > {threshold}\")\n",
    "   \n",
    "   return \"\".join(output)\n",
    "\n",
    "display(HTML(f\"<pre>{analyze_variance_threshold(threshold=0.01)}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 6 - FISHER SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fisher_score():\n",
    "    # Get numeric features excluding the target\n",
    "    numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "    numeric_features = [col for col in numeric_data.columns if col != 'rating']\n",
    "    \n",
    "    # Function to calculate Fisher score for a single feature\n",
    "    def calculate_fisher_score(feature):\n",
    "        # Calculate mean and variance for each class\n",
    "        class_stats = train_df.groupby('rating').agg({\n",
    "            feature: ['mean', 'var', 'count']\n",
    "        })[feature]\n",
    "        \n",
    "        # Overall mean of the feature\n",
    "        overall_mean = train_df[feature].mean()\n",
    "        \n",
    "        # Calculate between-class and within-class variance\n",
    "        between_class = sum(class_stats['count'] * (class_stats['mean'] - overall_mean) ** 2)\n",
    "        within_class = sum((class_stats['count'] - 1) * class_stats['var'])\n",
    "        \n",
    "        # Fisher score is between-class variance divided by within-class variance\n",
    "        if within_class == 0:\n",
    "            return 0  # To handle division by zero\n",
    "        return between_class / within_class\n",
    "    \n",
    "    # Calculate Fisher scores for all features\n",
    "    fisher_scores = {}\n",
    "    for feature in numeric_features:\n",
    "        fisher_scores[feature] = calculate_fisher_score(feature)\n",
    "    \n",
    "    # Sort scores in descending order\n",
    "    sorted_scores = sorted(fisher_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Prepare output\n",
    "    output = []\n",
    "    output.append(\"Fisher Score Analysis\")\n",
    "    output.append(\"=\" * 50)\n",
    "    output.append(\"Feature Scores (sorted by discriminative power):\")\n",
    "    output.append(\"-\" * 50)\n",
    "    \n",
    "    # Add interpretation based on score magnitude\n",
    "    for feature, score in sorted_scores:\n",
    "        if score > 1.0:\n",
    "            strength = \"strong\"\n",
    "        elif score > 0.5:\n",
    "            strength = \"moderate\"\n",
    "        elif score > 0.1:\n",
    "            strength = \"weak\"\n",
    "        else:\n",
    "            strength = \"very weak\"\n",
    "            \n",
    "        output.append(f\"{feature}: {score:.6f} ({strength} discriminative power)\")\n",
    "    \n",
    "    output.append(\"Interpretation:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    output.append(\"Higher Fisher scores indicate better discriminative power for rating prediction\")\n",
    "    output.append(\"• Strong: > 1.0\")\n",
    "    output.append(\"• Moderate: 0.5 - 1.0\")\n",
    "    output.append(\"• Weak: 0.1 - 0.5\")\n",
    "    output.append(\"• Very weak: < 0.1\")\n",
    "    \n",
    "    output.append(\"Recommendations:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    weak_features = [feature for feature, score in sorted_scores if score < 0.1]\n",
    "    if weak_features:\n",
    "        output.append(\"Consider removing these features with very weak discriminative power:\")\n",
    "        for feature in weak_features:\n",
    "            output.append(f\"- {feature}\")\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "display(HTML(f\"<pre>{analyze_fisher_score()}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 7 - MEAN ABSOLUTE DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_mean_absolute_difference():\n",
    "   # Get numeric features excluding the target\n",
    "   numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "   numeric_features = [col for col in numeric_data.columns if col != 'rating']\n",
    "   \n",
    "   # Function to calculate MAD for a single feature\n",
    "   def calculate_mad(feature):\n",
    "       feature_mean = train_df[feature].mean()\n",
    "       mad = (train_df[feature] - feature_mean).abs().mean()\n",
    "       return mad\n",
    "   \n",
    "   # Calculate MAD for all features\n",
    "   mad_scores = {}\n",
    "   for feature in numeric_features:\n",
    "       mad_scores[feature] = calculate_mad(feature)\n",
    "   \n",
    "   # Sort scores in descending order\n",
    "   sorted_scores = sorted(mad_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "   \n",
    "   # Prepare output\n",
    "   output = []\n",
    "   output.append(\"Mean Absolute Difference Analysis\")\n",
    "   output.append(\"=\" * 50)\n",
    "   output.append(\"Feature MAD Scores (sorted by variability):\")\n",
    "   output.append(\"-\" * 50)\n",
    "   \n",
    "   # Calculate mean MAD for threshold reference\n",
    "   mean_mad = np.mean(list(mad_scores.values()))\n",
    "   \n",
    "   # Add interpretation for each feature\n",
    "   for feature, score in sorted_scores:\n",
    "       if score > 2 * mean_mad:\n",
    "           variability = \"high\"\n",
    "       elif score > mean_mad:\n",
    "           variability = \"moderate\"\n",
    "       else:\n",
    "           variability = \"low\"\n",
    "           \n",
    "       output.append(f\"{feature}: {score:.6f} ({variability} variability)\")\n",
    "   \n",
    "   output.append(\"Interpretation:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   output.append(\"MAD measures the average absolute deviation from the mean\")\n",
    "   output.append(f\"Average MAD across features: {mean_mad:.6f}\")\n",
    "   output.append(\"• High variability: > 2x average MAD\")\n",
    "   output.append(\"• Moderate variability: > average MAD\")\n",
    "   output.append(\"• Low variability: ≤ average MAD\")\n",
    "   \n",
    "   output.append(\"Recommendations:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   low_var_features = [feature for feature, score in sorted_scores if score <= mean_mad/2]\n",
    "   if low_var_features:\n",
    "       output.append(\"Consider investigating these features with very low variability:\")\n",
    "       for feature in low_var_features:\n",
    "           output.append(f\"- {feature}\")\n",
    "   \n",
    "   return \"\".join(output)\n",
    "\n",
    "display(HTML(f\"<pre>{analyze_mean_absolute_difference()}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 8 - DISPERSION RATIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dispersion_ratio():\n",
    "   # Get numeric features excluding the target\n",
    "   numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "   numeric_features = [col for col in numeric_data.columns if col != 'rating']\n",
    "   \n",
    "   # Function to calculate dispersion ratio for a single feature\n",
    "   def calculate_dispersion_ratio(feature):\n",
    "       # Calculate dispersion (variance) for each rating group\n",
    "       group_vars = train_df.groupby('rating')[feature].var()\n",
    "       # Calculate overall variance\n",
    "       total_var = train_df[feature].var()\n",
    "       # Dispersion ratio is max group variance / min group variance\n",
    "       if group_vars.min() == 0:\n",
    "           return float('inf')  # Handle division by zero\n",
    "       return group_vars.max() / group_vars.min()\n",
    "   \n",
    "   # Calculate dispersion ratios for all features\n",
    "   dispersion_ratios = {}\n",
    "   for feature in numeric_features:\n",
    "       dispersion_ratios[feature] = calculate_dispersion_ratio(feature)\n",
    "   \n",
    "   # Sort ratios in descending order\n",
    "   sorted_ratios = sorted(dispersion_ratios.items(), key=lambda x: x[1], reverse=True)\n",
    "   \n",
    "   # Prepare output\n",
    "   output = []\n",
    "   output.append(\"Dispersion Ratio Analysis\")\n",
    "   output.append(\"=\" * 50)\n",
    "   output.append(\"Feature Dispersion Ratios (sorted by magnitude):\")\n",
    "   output.append(\"-\" * 50)\n",
    "   \n",
    "   # Add interpretation for each feature\n",
    "   for feature, ratio in sorted_ratios:\n",
    "       if ratio == float('inf'):\n",
    "           interpretation = \"infinite (contains zero variance group)\"\n",
    "       elif ratio > 10:\n",
    "           interpretation = \"very high dispersion\"\n",
    "       elif ratio > 5:\n",
    "           interpretation = \"high dispersion\"\n",
    "       elif ratio > 2:\n",
    "           interpretation = \"moderate dispersion\"\n",
    "       else:\n",
    "           interpretation = \"low dispersion\"\n",
    "           \n",
    "       output.append(f\"{feature}: {ratio if ratio != float('inf') else 'inf'} ({interpretation})\")\n",
    "   \n",
    "   output.append(\"Interpretation:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   output.append(\"Dispersion Ratio measures the spread variation across rating groups\")\n",
    "   output.append(\"• Very High: > 10\")\n",
    "   output.append(\"• High: 5 - 10\")\n",
    "   output.append(\"• Moderate: 2 - 5\")\n",
    "   output.append(\"• Low: < 2\")\n",
    "   \n",
    "   output.append(\"Recommendations:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   low_disp_features = [feature for feature, ratio in sorted_ratios if ratio < 2 and ratio != float('inf')]\n",
    "   if low_disp_features:\n",
    "       output.append(\"Features with low dispersion (might have limited discriminative power):\")\n",
    "       for feature in low_disp_features:\n",
    "           output.append(f\"- {feature}\")\n",
    "   \n",
    "   return \"\".join(output)\n",
    "\n",
    "display(HTML(f\"<pre>{analyze_dispersion_ratio()}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 9 - KRUSKAL-WALLIS TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "def analyze_kruskal_wallis(column_name):\n",
    "   if column_name not in train_df.columns:\n",
    "       return f\"Error: Column '{column_name}' not found in dataframe\"\n",
    "   \n",
    "   # Calculate test statistics\n",
    "   rating_groups = [train_df[train_df['rating'] == rating][column_name] \n",
    "                   for rating in train_df['rating'].unique()]\n",
    "   h_statistic, p_value = kruskal(*rating_groups)\n",
    "   \n",
    "   # Format output\n",
    "   output = []\n",
    "   output.append(f\"Kruskal-Wallis Test Analysis: rating vs {column_name}\")\n",
    "   output.append(\"=\" * 50)\n",
    "   \n",
    "   output.append(\"Hypothesis:\")\n",
    "   output.append(\"H0: The samples come from the same distribution (no difference between rating groups)\")\n",
    "   output.append(\"H1: The samples come from different distributions (difference exists between rating groups)\")\n",
    "   \n",
    "   output.append(\"Results:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   output.append(f\"H-statistic: {h_statistic:.4f}\")\n",
    "   output.append(f\"p-value: {p_value:.4f}\")\n",
    "   \n",
    "   # Interpretation\n",
    "   output.append(\"Interpretation:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   if p_value < 0.05:\n",
    "       output.append(\"p-value < 0.05: Reject H0\")\n",
    "       output.append(f\"There is a statistically significant difference in {column_name} across rating groups\")\n",
    "       \n",
    "       # Effect size interpretation (using H-statistic)\n",
    "       if h_statistic > 50:\n",
    "           strength = \"strong\"\n",
    "       elif h_statistic > 30:\n",
    "           strength = \"moderate\"\n",
    "       else:\n",
    "           strength = \"weak\"\n",
    "       output.append(f\"The effect size appears to be {strength} (H = {h_statistic:.2f})\")\n",
    "   else:\n",
    "       output.append(\"p-value ≥ 0.05: Fail to reject H0\")\n",
    "       output.append(f\"No significant difference in {column_name} across rating groups\")\n",
    "   \n",
    "   return \"\".join(output)\n",
    "\n",
    "# Get numeric columns excluding 'rating'\n",
    "numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'rating']\n",
    "\n",
    "print(\"Starting Kruskal-Wallis analysis for all numeric variables...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Loop through each numeric column\n",
    "for column in numeric_columns:\n",
    "   display(HTML(f\"<pre>{analyze_kruskal_wallis(column)}</pre>\"))\n",
    "   display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 10 - RECURSIVE FEATURE ELIMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_recursive_feature_elimination():\n",
    "    # Get numeric features excluding the target\n",
    "    numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "    X = numeric_data.drop('rating', axis=1)\n",
    "    y = train_df['rating']\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Initialize the estimator\n",
    "    estimator = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "    # Initialize RFECV\n",
    "    selector = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=1,\n",
    "        min_features_to_select=1,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    selector = selector.fit(X_scaled, y)\n",
    "\n",
    "    # Get selected features and their ranks\n",
    "    feature_ranks = dict(zip(X.columns, selector.ranking_))\n",
    "    selected_features = X.columns[selector.support_].tolist()\n",
    "\n",
    "    # Sort features by importance rank\n",
    "    sorted_features = sorted(feature_ranks.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Prepare output\n",
    "    output = []\n",
    "    output.append(\"Recursive Feature Elimination Analysis\")\n",
    "    output.append(\"=\" * 50)\n",
    "    output.append(f\"Optimal number of features: {selector.n_features_}\")\n",
    "    \n",
    "    # Use cv_results_ instead of grid_scores_\n",
    "    best_score = max(selector.cv_results_['mean_test_score'])\n",
    "    output.append(f\"Best CV score: {best_score:.4f}\")\n",
    "\n",
    "    output.append(\"Feature Rankings (1 = selected, higher = eliminated earlier):\")\n",
    "    output.append(\"-\" * 50)\n",
    "    for feature, rank in sorted_features:\n",
    "        status = \"selected\" if rank == 1 else \"eliminated\"\n",
    "        output.append(f\"{feature}: {rank} ({status})\")\n",
    "\n",
    "    output.append(\"Selected Features:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    for feature in selected_features:\n",
    "        output.append(f\"- {feature}\")\n",
    "\n",
    "    output.append(\"Recommendations:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    eliminated_features = [feature for feature, rank in feature_ranks.items() if rank > 1]\n",
    "    if eliminated_features:\n",
    "        output.append(\"Consider removing these features:\")\n",
    "        for feature in eliminated_features:\n",
    "            output.append(f\"- {feature}\")\n",
    "\n",
    "    return \"\".join(output)\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(f\"<pre>{analyze_recursive_feature_elimination()}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 11 - LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_lasso_feature_selection():\n",
    "   # Get numeric features excluding the target\n",
    "   numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "   X = numeric_data.drop('rating', axis=1)\n",
    "   y = train_df['rating']\n",
    "   \n",
    "   # Scale the features\n",
    "   scaler = StandardScaler()\n",
    "   X_scaled = scaler.fit_transform(X)\n",
    "   \n",
    "   # Initialize and fit LassoCV\n",
    "   lasso = LassoCV(\n",
    "       cv=5,\n",
    "       random_state=42,\n",
    "       max_iter=1000\n",
    "   )\n",
    "   lasso.fit(X_scaled, y)\n",
    "   \n",
    "   # Get feature coefficients\n",
    "   feature_coef = dict(zip(X.columns, lasso.coef_))\n",
    "   \n",
    "   # Sort features by absolute coefficient values\n",
    "   sorted_features = sorted(feature_coef.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "   \n",
    "   # Prepare output\n",
    "   output = []\n",
    "   output.append(\"Lasso Feature Selection Analysis\")\n",
    "   output.append(\"=\" * 50)\n",
    "   \n",
    "   output.append(f\"Best alpha: {lasso.alpha_:.6f}\")\n",
    "   \n",
    "   output.append(\"Feature Coefficients (sorted by importance):\")\n",
    "   output.append(\"-\" * 50)\n",
    "   \n",
    "   # Categorize features by importance\n",
    "   for feature, coef in sorted_features:\n",
    "       if abs(coef) == 0:\n",
    "           importance = \"not important (eliminated)\"\n",
    "       elif abs(coef) < 0.01:\n",
    "           importance = \"very weak importance\"\n",
    "       elif abs(coef) < 0.1:\n",
    "           importance = \"weak importance\"\n",
    "       elif abs(coef) < 0.5:\n",
    "           importance = \"moderate importance\"\n",
    "       else:\n",
    "           importance = \"strong importance\"\n",
    "           \n",
    "       output.append(f\"{feature}: {coef:.6f} ({importance})\")\n",
    "   \n",
    "   # Selected features (non-zero coefficients)\n",
    "   selected_features = [feature for feature, coef in sorted_features if coef != 0]\n",
    "   \n",
    "   output.append(\"Selected Features:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   output.append(f\"Number of features selected: {len(selected_features)}\")\n",
    "   for feature in selected_features:\n",
    "       output.append(f\"- {feature}\")\n",
    "   \n",
    "   output.append(\"Recommendations:\")\n",
    "   output.append(\"-\" * 50)\n",
    "   eliminated_features = [feature for feature, coef in sorted_features if coef == 0]\n",
    "   if eliminated_features:\n",
    "       output.append(\"Consider removing these features (zero coefficients):\")\n",
    "       for feature in eliminated_features:\n",
    "           output.append(f\"- {feature}\")\n",
    "   \n",
    "   weak_features = [feature for feature, coef in sorted_features \n",
    "                   if 0 < abs(coef) < 0.01]\n",
    "   if weak_features:\n",
    "       output.append(\"Features with very weak importance:\")\n",
    "       for feature in weak_features:\n",
    "           output.append(f\"- {feature}\")\n",
    "   \n",
    "   return \"\".join(output)\n",
    "\n",
    "display(HTML(f\"<pre>{analyze_lasso_feature_selection()}</pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 12 - RANDOM FOREST IMPORTANCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_random_forest_importance():\n",
    "    # Get numeric features excluding the target\n",
    "    numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "    X = numeric_data.drop('rating', axis=1)\n",
    "    y = train_df['rating']\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize and fit Random Forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_scaled, y)\n",
    "    \n",
    "    # Get feature importance scores\n",
    "    importance_scores = rf.feature_importances_\n",
    "    \n",
    "    # Create DataFrame with features and their importance scores\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': importance_scores\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Calculate cumulative importance\n",
    "    feature_importance['cumulative_importance'] = np.cumsum(feature_importance['importance'])\n",
    "    \n",
    "    # Find features that account for 95% of cumulative importance\n",
    "    important_features = feature_importance[\n",
    "        feature_importance['cumulative_importance'] <= 0.95\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    # Prepare output\n",
    "    output = []\n",
    "    output.append(\"Random Forest Feature Importance Analysis\")\n",
    "    output.append(\"=\" * 50)\n",
    "    \n",
    "    # Model performance\n",
    "    output.append(\"Model Performance:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    train_score = rf.score(X_scaled, y)\n",
    "    output.append(f\"Training accuracy: {train_score:.4f}\")\n",
    "    \n",
    "    # Feature importance details\n",
    "    output.append(\"Feature Importance Rankings:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    for idx, row in feature_importance.iterrows():\n",
    "        output.append(\n",
    "            f\"{row['feature']}: \"\n",
    "            f\"{row['importance']:.4f} \"\n",
    "            f\"(Cumulative: {row['cumulative_importance']:.4f})\"\n",
    "        )\n",
    "    \n",
    "    # Features accounting for 95% of importance\n",
    "    output.append(\"Key Features (95% of cumulative importance):\")\n",
    "    output.append(\"-\" * 50)\n",
    "    for feature in important_features:\n",
    "        output.append(f\"- {feature}\")\n",
    "    \n",
    "    # Low importance features\n",
    "    low_importance_features = feature_importance[\n",
    "        ~feature_importance['feature'].isin(important_features)\n",
    "    ]['feature'].tolist()\n",
    "    \n",
    "    output.append(\"Recommendations:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    if low_importance_features:\n",
    "        output.append(\"Consider removing these low-importance features:\")\n",
    "        for feature in low_importance_features:\n",
    "            importance = feature_importance[\n",
    "                feature_importance['feature'] == feature\n",
    "            ]['importance'].values[0]\n",
    "            output.append(f\"- {feature} (importance: {importance:.4f})\")\n",
    "    \n",
    "    # Additional insights\n",
    "    output.append(\"Additional Insights:\")\n",
    "    output.append(\"-\" * 50)\n",
    "    output.append(f\"Top 3 most important features account for \"\n",
    "                 f\"{feature_importance['importance'][:3].sum():.1%} of total importance\")\n",
    "    output.append(f\"Number of features needed for 95% importance: {len(important_features)}\")\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "# Run the analysis\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(f\"<pre>{analyze_random_forest_importance()}</pre>\"))\n",
    "\n",
    "# Optional: Create visualization of feature importance\n",
    "def plot_feature_importance():\n",
    "    \"\"\"\n",
    "    Create a visualization of feature importance\n",
    "    Returns a matplotlib figure\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Run the analysis to get feature importance\n",
    "    numeric_data = train_df.select_dtypes(include=['int64', 'float64'])\n",
    "    X = numeric_data.drop('rating', axis=1)\n",
    "    y = train_df['rating']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    \n",
    "    # Create feature importance DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Random Forest Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "display(plot_feature_importance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 13 - LOGISTIC REGRESSION WITH RIB RATIO STUDY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic regression with rib ratio study\n",
    "\n",
    "\n",
    "def analyze_variable_importance(column_name):\n",
    "    if column_name not in train_df.columns:\n",
    "        return f\"Error: Column '{column_name}' not found in dataframe\"\n",
    "    \n",
    "    y = (train_df['rating'] >= 4).astype(int)\n",
    "    \n",
    "    X = train_df[column_name]\n",
    "    \n",
    "    if not np.issubdtype(X.dtype, np.number):\n",
    "        return f\"Error: Column '{column_name}' is not numeric. Please encode categorical variables first.\"\n",
    "    \n",
    "    X = X.values.reshape(-1, 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit logistic regression\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Calculate correlation (point biserial for binary vs continuous)\n",
    "    correlation, p_value = stats.pointbiserialr(y, X.ravel())\n",
    "    \n",
    "    output = []\n",
    "    output.append(f\"Variable Importance Analysis: {column_name} vs High/Low Rating\")\n",
    "    output.append(\"=\" * 60)\n",
    "    \n",
    "    # Basic statistics\n",
    "    output.append(\"Basic Statistics:\")\n",
    "    output.append(f\"Mean value for low ratings: {X[y==0].mean():.3f}\")\n",
    "    output.append(f\"Mean value for high ratings: {X[y==1].mean():.3f}\")\n",
    "    output.append(f\"Correlation coefficient: {correlation:.3f}\")\n",
    "    output.append(f\"P-value: {p_value:.3e}\")\n",
    "    \n",
    "    # Logistic Regression Results\n",
    "    output.append(\"Logistic Regression Results:\")\n",
    "    output.append(f\"Coefficient: {model.coef_[0][0]:.3f}\")\n",
    "    output.append(f\"Intercept: {model.intercept_[0]:.3f}\")\n",
    "    output.append(f\"ROC AUC Score: {roc_auc:.3f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    output.append(\"Classification Report:\")\n",
    "    output.append(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Effect interpretation\n",
    "    output.append(\"Interpretation:\")\n",
    "    if abs(correlation) < 0.1:\n",
    "        strength = \"negligible\"\n",
    "    elif abs(correlation) < 0.3:\n",
    "        strength = \"weak\"\n",
    "    elif abs(correlation) < 0.5:\n",
    "        strength = \"moderate\"\n",
    "    else:\n",
    "        strength = \"strong\"\n",
    "    \n",
    "    direction = \"positive\" if correlation > 0 else \"negative\"\n",
    "    \n",
    "    output.append(f\"- The relationship is {strength} and {direction}\")\n",
    "    if p_value < 0.05:\n",
    "        output.append(\"- The relationship is statistically significant\")\n",
    "        output.append(f\"- As {column_name} increases, the likelihood of a high rating\")\n",
    "        output.append(f\"  {'increases' if correlation > 0 else 'decreases'}\")\n",
    "    else:\n",
    "        output.append(\"- The relationship is not statistically significant\")\n",
    "    \n",
    "    output.append(f\"- The model has an ROC AUC of {roc_auc:.3f}, indicating\")\n",
    "    if roc_auc < 0.6:\n",
    "        output.append(\"  poor predictive power\")\n",
    "    elif roc_auc < 0.7:\n",
    "        output.append(\"  fair predictive power\")\n",
    "    elif roc_auc < 0.8:\n",
    "        output.append(\"  good predictive power\")\n",
    "    else:\n",
    "        output.append(\"  excellent predictive power\")\n",
    "    \n",
    "    output.append(\"Keep for model training: \")\n",
    "    output.append(\"Yes\" if (p_value < 0.05 and roc_auc > 0.6) else \"Consider dropping\")\n",
    "    \n",
    "    return \"\".join(output)\n",
    "\n",
    "# Get numeric columns excluding rating\n",
    "numeric_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_columns = [col for col in numeric_columns if col != 'rating']\n",
    "\n",
    "print(\"Starting variable importance analysis...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Loop through each numeric column\n",
    "for column in numeric_columns:\n",
    "    display(HTML(f\"<pre>{analyze_variable_importance(column)}</pre>\"))\n",
    "    display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 14 - VALIDATE AND COMPARE METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def collect_all_scores(df, target_col='rating'):\n",
    "    # Get feature columns (numeric only)\n",
    "    features = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    features = [col for col in features if col != target_col]\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    scores = {\n",
    "        'feature': features,\n",
    "        'pearson': [],\n",
    "        'spearman': [],\n",
    "        'mutual_info': [],\n",
    "        'random_forest': [],\n",
    "        'lasso': []\n",
    "    }\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Calculate scores for each feature\n",
    "    for feature in features:\n",
    "        # Correlation scores\n",
    "        pearson_coef, _ = pearsonr(df[feature], y)\n",
    "        spearman_coef, _ = spearmanr(df[feature], y)\n",
    "        \n",
    "        scores['pearson'].append(abs(pearson_coef))\n",
    "        scores['spearman'].append(abs(spearman_coef))\n",
    "    \n",
    "    # Mutual Information\n",
    "    scores['mutual_info'] = mutual_info_regression(X, y)\n",
    "    \n",
    "    # Random Forest importance\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X_scaled, y)\n",
    "    scores['random_forest'] = rf.feature_importances_\n",
    "    \n",
    "    # Lasso importance\n",
    "    lasso = LassoCV(cv=5, random_state=42)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    scores['lasso'] = np.abs(lasso.coef_)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    return scores_df\n",
    "\n",
    "def normalize_scores(scores_df):\n",
    "    # Get feature column\n",
    "    features = scores_df['feature']\n",
    "    \n",
    "    # Normalize each method's scores\n",
    "    methods = ['pearson', 'spearman', 'mutual_info', 'random_forest', 'lasso']\n",
    "    normalized_scores = scores_df[methods].apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() != x.min() else x)\n",
    "    \n",
    "    # Add features back\n",
    "    normalized_scores.insert(0, 'feature', features)\n",
    "    \n",
    "    return normalized_scores\n",
    "\n",
    "def get_feature_recommendations(scores_df):\n",
    "    # Calculate mean and std of scores for each feature\n",
    "    score_cols = ['pearson', 'spearman', 'mutual_info', 'random_forest', 'lasso']\n",
    "    scores_df['mean_score'] = scores_df[score_cols].mean(axis=1)\n",
    "    scores_df['score_std'] = scores_df[score_cols].std(axis=1)\n",
    "    \n",
    "    # Generate recommendations\n",
    "    def get_recommendation(row):\n",
    "        if row['mean_score'] >= 0.7 and row['score_std'] < 0.2:\n",
    "            return \"Strong Keep\"\n",
    "        elif row['mean_score'] >= 0.4 and row['score_std'] < 0.3:\n",
    "            return \"Keep\"\n",
    "        elif row['mean_score'] >= 0.1 and row['score_std'] < 0.4:\n",
    "            return \"Consider Keeping\"\n",
    "        elif row['score_std'] >= 0.4:\n",
    "            return \"Review (Conflicting Results)\"\n",
    "        else:\n",
    "            return \"Consider Removing\"\n",
    "    \n",
    "    scores_df['recommendation'] = scores_df.apply(get_recommendation, axis=1)\n",
    "    return scores_df\n",
    "\n",
    "def create_summary(final_scores):\n",
    "    summary = []\n",
    "    summary.append(\"Feature Selection Analysis Summary\")\n",
    "    summary.append(\"=\" * 50)\n",
    "    \n",
    "    # Overall recommendations\n",
    "    summary.append(\"\\nRecommendations:\")\n",
    "    for recommendation in final_scores['recommendation'].unique():\n",
    "        features = final_scores[final_scores['recommendation'] == recommendation]\n",
    "        summary.append(f\"\\n{recommendation}:\")\n",
    "        for _, row in features.iterrows():\n",
    "            summary.append(f\"- {row['feature']} (mean score: {row['mean_score']:.3f})\")\n",
    "    \n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "def plot_importance_heatmap(scores_df):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    score_cols = ['pearson', 'spearman', 'mutual_info', 'random_forest', 'lasso']\n",
    "    heatmap_data = scores_df.set_index('feature')[score_cols]\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='YlOrRd', fmt='.2f')\n",
    "    plt.title('Feature Importance Across Methods')\n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def analyze_features(df, target_col='rating'):\n",
    "    # 1. Collect all scores\n",
    "    print(\"Collecting scores from all methods...\")\n",
    "    scores = collect_all_scores(df, target_col)\n",
    "    \n",
    "    # 2. Normalize scores\n",
    "    print(\"Normalizing scores...\")\n",
    "    normalized_scores = normalize_scores(scores)\n",
    "    \n",
    "    # 3. Generate recommendations\n",
    "    print(\"Generating recommendations...\")\n",
    "    final_scores = get_feature_recommendations(normalized_scores)\n",
    "    \n",
    "    # 4. Create summary\n",
    "    print(\"\\nAnalysis Summary:\")\n",
    "    summary = create_summary(final_scores)\n",
    "    display(HTML(f\"<pre>{summary}</pre>\"))\n",
    "    \n",
    "    # 5. Show detailed results\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    display(final_scores.style\n",
    "           .background_gradient(subset=['mean_score'])\n",
    "           .set_caption(\"Feature Selection Results\"))\n",
    "    \n",
    "    # 6. Show visualization\n",
    "    print(\"\\nFeature Importance Heatmap:\")\n",
    "    display(plot_importance_heatmap(normalized_scores))\n",
    "    \n",
    "    return final_scores\n",
    "\n",
    "results = analyze_features(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def analyze_final_features(df, target_col='rating', vif_threshold=5, corr_threshold=0.7):\n",
    "    # Initial feature sets before multicollinearity check\n",
    "    initial_features = {\n",
    "        'conservative': [\n",
    "            'verification', \n",
    "            'answered', \n",
    "            'repeat_reviewer_encoded',\n",
    "            'day_of_week_posted',\n",
    "            'day_type',\n",
    "            'review_time',\n",
    "            'month_posted',\n",
    "            'local_hour'\n",
    "        ],\n",
    "        'strict': [\n",
    "            'verification',\n",
    "            'answered',\n",
    "            'repeat_reviewer_encoded'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Check multicollinearity and get cleaned feature sets\n",
    "    print(\"Analyzing multicollinearity...\")\n",
    "    multi_results = check_multicollinearity(df, initial_features['conservative'], \n",
    "                                          target_col, vif_threshold, corr_threshold)\n",
    "    results['multicollinearity'] = multi_results\n",
    "    \n",
    "    # Update feature sets based on multicollinearity analysis\n",
    "    cleaned_features = multi_results['cleaned_features']\n",
    "    \n",
    "    # Update conservative and strict approaches\n",
    "    conservative_features = cleaned_features\n",
    "    strict_features = [f for f in cleaned_features if f in initial_features['strict']]\n",
    "    \n",
    "    print(\"\\nAfter multicollinearity handling:\")\n",
    "    print(f\"Conservative features: {conservative_features}\")\n",
    "    print(f\"Strict features: {strict_features}\")\n",
    "    \n",
    "    # 2. Validate both feature sets\n",
    "    print(\"\\nValidating feature sets...\")\n",
    "    conservative_metrics = validate_feature_set(df, conservative_features, target_col)\n",
    "    strict_metrics = validate_feature_set(df, strict_features, target_col)\n",
    "    \n",
    "    results['conservative'] = {\n",
    "        'features': conservative_features,\n",
    "        'metrics': conservative_metrics\n",
    "    }\n",
    "    results['strict'] = {\n",
    "        'features': strict_features,\n",
    "        'metrics': strict_metrics\n",
    "    }\n",
    "    \n",
    "    # 3. Generate final importance ranking for both approaches\n",
    "    print(\"\\nGenerating final importance ranking...\")\n",
    "    importance_ranking = get_final_importance_ranking(df, conservative_features, strict_features, target_col)\n",
    "    results['importance_ranking'] = importance_ranking\n",
    "    \n",
    "    # Display results\n",
    "    display_comprehensive_results(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_multicollinearity(df, features, target_col, vif_threshold=5, corr_threshold=0.7):\n",
    "    features_to_check = [f for f in features if f in df.columns]\n",
    "    X = df[features_to_check]\n",
    "    \n",
    "    # Calculate initial VIF\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "    \n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X_scaled.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) \n",
    "                       for i in range(X_scaled.shape[1])]\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = X.corr()\n",
    "    \n",
    "    # Calculate correlation with target for importance assessment\n",
    "    target_correlations = X.apply(lambda x: abs(x.corr(df[target_col])))\n",
    "    \n",
    "    # Find features to remove due to multicollinearity\n",
    "    features_to_remove = set()\n",
    "    \n",
    "    # Check VIF\n",
    "    high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"].tolist()\n",
    "    if high_vif_features:\n",
    "        # Remove features with highest VIF iteratively\n",
    "        while len(high_vif_features) > 0:\n",
    "            # Remove feature with highest VIF and lowest correlation with target\n",
    "            worst_feature = min(high_vif_features, \n",
    "                              key=lambda x: target_correlations[x])\n",
    "            features_to_remove.add(worst_feature)\n",
    "            \n",
    "            # Recalculate VIF without the removed feature\n",
    "            remaining_features = [f for f in X.columns if f not in features_to_remove]\n",
    "            if not remaining_features:\n",
    "                break\n",
    "                \n",
    "            X_subset = X[remaining_features]\n",
    "            X_scaled = pd.DataFrame(scaler.fit_transform(X_subset), \n",
    "                                  columns=X_subset.columns)\n",
    "            \n",
    "            vif_data = pd.DataFrame()\n",
    "            vif_data[\"Feature\"] = X_scaled.columns\n",
    "            vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) \n",
    "                              for i in range(X_scaled.shape[1])]\n",
    "            \n",
    "            high_vif_features = vif_data[vif_data[\"VIF\"] > vif_threshold][\"Feature\"].tolist()\n",
    "    \n",
    "    # Check correlations\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > corr_threshold:\n",
    "                feat1 = corr_matrix.columns[i]\n",
    "                feat2 = corr_matrix.columns[j]\n",
    "                corr = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                # Decide which feature to remove based on correlation with target\n",
    "                if target_correlations[feat1] < target_correlations[feat2]:\n",
    "                    features_to_remove.add(feat1)\n",
    "                else:\n",
    "                    features_to_remove.add(feat2)\n",
    "                    \n",
    "                high_corr_pairs.append({\n",
    "                    'feature1': feat1,\n",
    "                    'feature2': feat2,\n",
    "                    'correlation': corr,\n",
    "                    'removed': feat1 if target_correlations[feat1] < target_correlations[feat2] else feat2\n",
    "                })\n",
    "    \n",
    "    # Get final cleaned feature set\n",
    "    cleaned_features = [f for f in features if f not in features_to_remove]\n",
    "    \n",
    "    return {\n",
    "        'vif': vif_data,\n",
    "        'correlation_matrix': corr_matrix,\n",
    "        'high_correlations': high_corr_pairs,\n",
    "        'removed_features': list(features_to_remove),\n",
    "        'cleaned_features': cleaned_features\n",
    "    }\n",
    "\n",
    "def validate_feature_set(df, features, target_col):\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision': make_scorer(precision_score, average='weighted'),\n",
    "        'recall': make_scorer(recall_score, average='weighted'),\n",
    "        'f1': make_scorer(f1_score, average='weighted')\n",
    "    }\n",
    "    \n",
    "    metrics = {}\n",
    "    for metric_name, scorer in scoring.items():\n",
    "        cv_scores = cross_val_score(rf, X_scaled, y, scoring=scorer, cv=5)\n",
    "        metrics[metric_name] = {\n",
    "            'mean': cv_scores.mean(),\n",
    "            'std': cv_scores.std()\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def get_final_importance_ranking(df, conservative_features, strict_features, target_col):\n",
    "    X_conservative = df[conservative_features]\n",
    "    X_strict = df[strict_features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    \n",
    "    # Get conservative approach importance\n",
    "    X_cons_scaled = scaler.fit_transform(X_conservative)\n",
    "    rf.fit(X_cons_scaled, y)\n",
    "    conservative_importance = pd.DataFrame({\n",
    "        'feature': conservative_features,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Get strict approach importance\n",
    "    X_strict_scaled = scaler.fit_transform(X_strict)\n",
    "    rf.fit(X_strict_scaled, y)\n",
    "    strict_importance = pd.DataFrame({\n",
    "        'feature': strict_features,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'conservative': conservative_importance,\n",
    "        'strict': strict_importance\n",
    "    }\n",
    "\n",
    "def display_comprehensive_results(results):\n",
    "\n",
    "    print(\"\\nMulticollinearity Analysis Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Display removed features\n",
    "    print(\"\\nFeatures removed due to multicollinearity:\")\n",
    "    if results['multicollinearity']['removed_features']:\n",
    "        for feature in results['multicollinearity']['removed_features']:\n",
    "            print(f\"- {feature}\")\n",
    "    else:\n",
    "        print(\"No features removed\")\n",
    "    \n",
    "    # Display high correlations\n",
    "    print(\"\\nHighly Correlated Feature Pairs:\")\n",
    "    if results['multicollinearity']['high_correlations']:\n",
    "        for pair in results['multicollinearity']['high_correlations']:\n",
    "            print(f\"{pair['feature1']} & {pair['feature2']}: {pair['correlation']:.3f}\")\n",
    "            print(f\"Removed: {pair['removed']}\")\n",
    "    else:\n",
    "        print(\"No highly correlated pairs found\")\n",
    "    \n",
    "    # Plot correlation heatmap for final features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(results['multicollinearity']['correlation_matrix'], \n",
    "                annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display validation results\n",
    "    print(\"\\nFeature Set Validation Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nConservative Approach:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Features:\", \", \".join(results['conservative']['features']))\n",
    "    for metric, scores in results['conservative']['metrics'].items():\n",
    "        print(f\"{metric}: {scores['mean']:.3f} (±{scores['std']:.3f})\")\n",
    "    \n",
    "    print(\"\\nStrict Approach:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Features:\", \", \".join(results['strict']['features']))\n",
    "    for metric, scores in results['strict']['metrics'].items():\n",
    "        print(f\"{metric}: {scores['mean']:.3f} (±{scores['std']:.3f})\")\n",
    "    \n",
    "    # Display feature importance rankings\n",
    "    print(\"\\nFeature Importance Rankings:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nConservative Approach Importance:\")\n",
    "    print(\"-\" * 30)\n",
    "    display(results['importance_ranking']['conservative'])\n",
    "    \n",
    "    print(\"\\nStrict Approach Importance:\")\n",
    "    print(\"-\" * 30)\n",
    "    display(results['importance_ranking']['strict'])\n",
    "    \n",
    "    # Compare relative importance changes\n",
    "    common_features = set(results['importance_ranking']['strict']['feature']) & \\\n",
    "                     set(results['importance_ranking']['conservative']['feature'])\n",
    "    \n",
    "    if common_features:\n",
    "        print(\"\\nFeature Importance Comparison (Strict vs Conservative):\")\n",
    "        print(\"-\" * 30)\n",
    "        for feature in common_features:\n",
    "            strict_imp = results['importance_ranking']['strict'][\n",
    "                results['importance_ranking']['strict']['feature'] == feature]['importance'].iloc[0]\n",
    "            cons_imp = results['importance_ranking']['conservative'][\n",
    "                results['importance_ranking']['conservative']['feature'] == feature]['importance'].iloc[0]\n",
    "            print(f\"{feature}:\")\n",
    "            print(f\"  Strict: {strict_imp:.3f}\")\n",
    "            print(f\"  Conservative: {cons_imp:.3f}\")\n",
    "            print(f\"  Change: {(strict_imp - cons_imp):.3f}\")\n",
    "\n",
    "\n",
    "results = analyze_final_features(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 15 - DIMENSION REDUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not needed as we have less than 20 features in our final selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 16 - SCALING/NORMALIZATION/STANDARDIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def scale_features(train_df, test_df):\n",
    "    # Identify columns that need scaling\n",
    "    columns_to_scale = ['local_hour', 'time_of_day', 'day_of_week_posted', 'review_time']\n",
    "    \n",
    "    # Binary columns (0/1) - don't scale these\n",
    "    binary_columns = ['verification', 'answered', 'repeat_reviewer_encoded', 'day_type']\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Create copies of the dataframes\n",
    "    train_scaled = train_df.copy()\n",
    "    test_scaled = test_df.copy()\n",
    "    \n",
    "    # Scale only the selected columns\n",
    "    train_scaled[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])\n",
    "    test_scaled[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "    \n",
    "    return train_scaled, test_scaled, scaler\n",
    "\n",
    "# Use the function\n",
    "train_scaled, test_scaled, scaler = scale_features(train_df, test_df)\n",
    "\n",
    "# Verify scaling results\n",
    "print(\"\\nTraining set statistics after scaling:\")\n",
    "print(train_scaled[['local_hour', 'time_of_day', 'day_of_week_posted', 'review_time']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 17 - OVERSAMPLING/UNDERSAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Class distribution in training set:\")\n",
    "print(train_scaled['rating'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def apply_balancing_methods(X_train, y_train, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply different balancing methods without external packages\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Store original data\n",
    "    results['original'] = (X_train.copy(), y_train.copy())\n",
    "    \n",
    "    # 1. Random Oversampling\n",
    "    # Identify minority classes and oversample each\n",
    "    X_over = pd.DataFrame()\n",
    "    y_over = pd.Series()\n",
    "    target_size = y_train.value_counts().max()\n",
    "    \n",
    "    for class_value in y_train.unique():\n",
    "        # Get indices for this class\n",
    "        class_indices = y_train == class_value\n",
    "        X_class = X_train[class_indices]\n",
    "        y_class = y_train[class_indices]\n",
    "        \n",
    "        # If this is a minority class, oversample it\n",
    "        if len(y_class) < target_size:\n",
    "            X_resampled = resample(X_class, \n",
    "                                 n_samples=target_size,\n",
    "                                 random_state=random_state)\n",
    "            y_resampled = pd.Series([class_value] * target_size)\n",
    "        else:\n",
    "            X_resampled = X_class\n",
    "            y_resampled = y_class\n",
    "            \n",
    "        X_over = pd.concat([X_over, X_resampled])\n",
    "        y_over = pd.concat([y_over, y_resampled])\n",
    "    \n",
    "    results['oversampling'] = (X_over, y_over)\n",
    "    \n",
    "    # 2. Random Undersampling\n",
    "    # Undersample majority classes to match minority class size\n",
    "    X_under = pd.DataFrame()\n",
    "    y_under = pd.Series()\n",
    "    target_size = y_train.value_counts().min()\n",
    "    \n",
    "    for class_value in y_train.unique():\n",
    "        # Get indices for this class\n",
    "        class_indices = y_train == class_value\n",
    "        X_class = X_train[class_indices]\n",
    "        y_class = y_train[class_indices]\n",
    "        \n",
    "        # If this is a majority class, undersample it\n",
    "        if len(y_class) > target_size:\n",
    "            X_resampled = resample(X_class,\n",
    "                                 n_samples=target_size,\n",
    "                                 random_state=random_state)\n",
    "            y_resampled = pd.Series([class_value] * target_size)\n",
    "        else:\n",
    "            X_resampled = X_class\n",
    "            y_resampled = y_class\n",
    "            \n",
    "        X_under = pd.concat([X_under, X_resampled])\n",
    "        y_under = pd.concat([y_under, y_resampled])\n",
    "    \n",
    "    results['undersampling'] = (X_under, y_under)\n",
    "    \n",
    "    # 3. Class Weights\n",
    "    # Calculate class weights inversely proportional to class frequencies\n",
    "    class_weights = dict(zip(\n",
    "        np.unique(y_train),\n",
    "        1/(np.bincount(y_train)[np.unique(y_train)])\n",
    "    ))\n",
    "    results['class_weights'] = class_weights\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_distributions(results):\n",
    "    \"\"\"Print class distributions for each method\"\"\"\n",
    "    print(\"Original class distribution:\")\n",
    "    print(results['original'][1].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nOversampled class distribution:\")\n",
    "    print(results['oversampling'][1].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nUndersampled class distribution:\")\n",
    "    print(results['undersampling'][1].value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nClass weights for weighted approach:\")\n",
    "    print(results['class_weights'])\n",
    "\n",
    "# Apply the balancing methods\n",
    "X = train_scaled.drop('rating', axis=1)\n",
    "y = train_scaled['rating']\n",
    "\n",
    "# Get balanced datasets\n",
    "results = apply_balancing_methods(X, y)\n",
    "\n",
    "# Print distributions\n",
    "print_distributions(results)\n",
    "\n",
    "# Store results for later use\n",
    "balanced_datasets = {\n",
    "    'original': results['original'],\n",
    "    'oversampling': results['oversampling'],\n",
    "    'undersampling': results['undersampling'],\n",
    "    'class_weights': results['class_weights']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAGE 18 - CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_feature_set(X, y, balanced_datasets, feature_set_name, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate one feature set (Conservative or Strict) with different preprocessing methods\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Try each sampling method\n",
    "    for sampling_method in ['original', 'oversampling', 'undersampling']:\n",
    "        # Get the appropriate dataset\n",
    "        X_sampled = balanced_datasets[sampling_method][0][X.columns]\n",
    "        y_sampled = balanced_datasets[sampling_method][1]\n",
    "        \n",
    "        # Basic Random Forest without class weights\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=random_state)\n",
    "        scores = cross_val_score(rf, X_sampled, y_sampled, cv=5, scoring='accuracy')\n",
    "        \n",
    "        results.append({\n",
    "            'Preprocessing Method': sampling_method,\n",
    "            'Weight Method': 'None',\n",
    "            'Mean CV Score': scores.mean(),\n",
    "            'Std CV Score': scores.std()\n",
    "        })\n",
    "    \n",
    "    # Try with class weights\n",
    "    rf_weighted = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        class_weight=balanced_datasets['class_weights'],\n",
    "        random_state=random_state\n",
    "    )\n",
    "    scores = cross_val_score(rf_weighted, X, y, cv=5, scoring='accuracy')\n",
    "    \n",
    "    results.append({\n",
    "        'Preprocessing Method': 'original',\n",
    "        'Weight Method': 'class_weights',\n",
    "        'Mean CV Score': scores.mean(),\n",
    "        'Std CV Score': scores.std()\n",
    "    })\n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values('Mean CV Score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n{feature_set_name} Feature Set Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(results_df)\n",
    "    \n",
    "    best_method = results_df.iloc[0]\n",
    "    print(f\"\\nBest preprocessing for {feature_set_name}:\")\n",
    "    print(f\"Method: {best_method['Preprocessing Method']}\")\n",
    "    print(f\"Weights: {best_method['Weight Method']}\")\n",
    "    print(f\"Score: {best_method['Mean CV Score']:.3f} (±{best_method['Std CV Score']:.3f})\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Prepare conservative and strict feature sets\n",
    "conservative_features = ['verification', 'answered', 'repeat_reviewer_encoded', \n",
    "                        'day_of_week_posted', 'day_type', 'review_time', \n",
    "                        'month_posted', 'local_hour']\n",
    "\n",
    "strict_features = ['verification', 'answered', 'repeat_reviewer_encoded']\n",
    "\n",
    "# Evaluate each feature set separately\n",
    "X_conservative = train_scaled[conservative_features]\n",
    "X_strict = train_scaled[strict_features]\n",
    "y = train_scaled['rating']\n",
    "\n",
    "# Get results for both approaches\n",
    "conservative_results = evaluate_feature_set(X_conservative, y, balanced_datasets, \"Conservative\")\n",
    "strict_results = evaluate_feature_set(X_strict, y, balanced_datasets, \"Strict\")\n",
    "\n",
    "# Compare best results from each feature set\n",
    "print(\"\\nFinal Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "best_conservative = conservative_results.iloc[0]\n",
    "best_strict = strict_results.iloc[0]\n",
    "\n",
    "print(\"\\nBest Conservative Setup:\")\n",
    "print(f\"Preprocessing: {best_conservative['Preprocessing Method']}\")\n",
    "print(f\"Weights: {best_conservative['Weight Method']}\")\n",
    "print(f\"Score: {best_conservative['Mean CV Score']:.3f} (±{best_conservative['Std CV Score']:.3f})\")\n",
    "\n",
    "print(\"\\nBest Strict Setup:\")\n",
    "print(f\"Preprocessing: {best_strict['Preprocessing Method']}\")\n",
    "print(f\"Weights: {best_strict['Weight Method']}\")\n",
    "print(f\"Score: {best_strict['Mean CV Score']:.3f} (±{best_strict['Std CV Score']:.3f})\")\n",
    "\n",
    "if best_conservative['Mean CV Score'] > best_strict['Mean CV Score']:\n",
    "    winner = \"Conservative\"\n",
    "    margin = best_conservative['Mean CV Score'] - best_strict['Mean CV Score']\n",
    "else:\n",
    "    winner = \"Strict\"\n",
    "    margin = best_strict['Mean CV Score'] - best_conservative['Mean CV Score']\n",
    "\n",
    "print(f\"\\nOverall Winner: {winner} feature set\")\n",
    "print(f\"Performance margin: {margin:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Determine winning approach from cross-validation\n",
    "best_conservative = conservative_results.iloc[0]\n",
    "best_strict = strict_results.iloc[0]\n",
    "\n",
    "# Save the winning approach\n",
    "if best_conservative['Mean CV Score'] > best_strict['Mean CV Score']:\n",
    "    winning_cols = ['verification', 'answered', 'repeat_reviewer_encoded',\n",
    "                   'day_of_week_posted', 'day_type', 'review_time',\n",
    "                   'month_posted', 'local_hour', 'rating']\n",
    "else:\n",
    "    winning_cols = ['verification', 'answered', 'repeat_reviewer_encoded', 'rating']\n",
    "\n",
    "# Save datasets\n",
    "df_trustpilot.to_csv('data/data_trustpilot_4.csv', index=False)\n",
    "train_df[winning_cols].to_csv('data/train_4.csv', index=False)\n",
    "test_df[winning_cols].to_csv('data/test_4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
